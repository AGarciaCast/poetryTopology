{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q selenium\n",
    "#pip install ripser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import time, random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import stablerank.srank as sr\n",
    "from ripser import ripser\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import scipy.spatial as spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"./Data\"\n",
    "\n",
    "def extract_text(path):\n",
    "    poets, titles, poems = [], [], []\n",
    "    in_poem = False\n",
    "    poet = False\n",
    "    title = False \n",
    "    poem = \"\"\n",
    "    with open(path, \"r\", encoding=\"UTF-8\") as f:\n",
    "        for line in f:\n",
    "            if  line.strip() == \"******\":\n",
    "                poet = False\n",
    "                title = False\n",
    "                poems.append(poem.strip())\n",
    "                poem = \"\"\n",
    "            elif poet and title:\n",
    "                poem += line\n",
    "                in_poem = True\n",
    "            elif not title:\n",
    "                titles.append(line.strip())\n",
    "                title = True\n",
    "            elif not poet:\n",
    "                poets.append(line.strip())\n",
    "                poet = True\n",
    "    \n",
    "    return pd.DataFrame({\"Poet\":poets, \"Poem\":poems, \"Title\":titles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_poem(driver):\n",
    "    elem = driver.find_element(By.CLASS_NAME, \"card-body\")\n",
    "    return \"\\n\\n\".join(p.text for p in elem.find_elements(By.CSS_SELECTOR, \"p\"))\n",
    "\n",
    "def iterate_poems(driver, df=None):\n",
    "\n",
    "    def get_elements():\n",
    "        mytable = driver.find_element(By.CSS_SELECTOR, 'tbody')\n",
    "        return mytable.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "    poems = []\n",
    "    titles = []\n",
    "    auth_elem = driver.find_element(By.CLASS_NAME, \"poet__name\")\n",
    "    author = auth_elem.text\n",
    "    main_window = driver.current_window_handle\n",
    "\n",
    "    i = 0\n",
    "    elements = get_elements()\n",
    "    for link in elements:\n",
    "        title = link.text\n",
    "        if df is not None and title in df[\"Title\"].values:\n",
    "            continue\n",
    "        \n",
    "        titles.append(title)\n",
    "\n",
    "        # Open link in new tab\n",
    "        link.send_keys(Keys.CONTROL + Keys.RETURN)\n",
    "        windows = driver.window_handles\n",
    "        driver.switch_to.window(windows[-1])\n",
    "\n",
    "        # Extract poem\n",
    "        time.sleep(2)\n",
    "        poems.append(extract_poem(driver))\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Close Current Tab\n",
    "        driver.close()\n",
    "\n",
    "        # Put focus back on main window\n",
    "        driver.switch_to.window(main_window)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    ActionChains(driver).move_to_element(auth_elem).perform()\n",
    "\n",
    "    return author, titles, poems\n",
    "\n",
    "def iterate_web(driver, web, df=None, max_p=5):\n",
    "    driver.get(web)\n",
    "    time.sleep(1)\n",
    "\n",
    "    more_next = True\n",
    "    if df is None:\n",
    "        df = pd.DataFrame({\"Poet\":[], \"Poem\":[], \"Title\":[]})\n",
    "        \n",
    "    i=0\n",
    "    while more_next and i < max_p:\n",
    "        # Obtain all poems author\n",
    "        author, titles, poems = iterate_poems(driver, df)\n",
    "\n",
    "        df_aux = pd.DataFrame({\"Poet\":[author]*len(poems),\n",
    "                                \"Poem\":poems,\n",
    "                                \"Title\":titles})\n",
    "\n",
    "        df = pd.concat([df, df_aux], ignore_index=True)\n",
    "        del df_aux\n",
    "\n",
    "        try:\n",
    "            time.sleep(5)\n",
    "            # Remove spam covering next\n",
    "            link = driver.find_element(By.XPATH,\n",
    "                                        '/html/body/w-div/span')\n",
    "            link.click()\n",
    "            time.sleep(0.5)\n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            pass\n",
    "\n",
    "        try:     \n",
    "            # Click next   \n",
    "            link = driver.find_element(By.CSS_SELECTOR,\n",
    "                                        '[aria-label=\"Go to next page\"]')\n",
    "            link.click()\n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            more_next = False\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return df\n",
    "\n",
    "def extract_webs(webs, df=None):\n",
    "    driver = webdriver.Chrome()\n",
    "    if df is None:\n",
    "        df = pd.DataFrame({\"Poet\":[], \"Poem\":[], \"Title\":[]})\n",
    "        \n",
    "    for web in webs:\n",
    "        df = iterate_web(driver, web, df)\n",
    "        df.to_csv(os.path.join(path_data, \"PoetryData.csv\"),index=False)\n",
    "    \n",
    "    #time.sleep(50)\n",
    "    driver.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://poets.org/poet/e-e-cummings']\n"
     ]
    }
   ],
   "source": [
    "webs=[]\n",
    "with open(os.path.join(path_data, \"poets.txt\"), \"r\") as f:\n",
    "        webs = [w for w in f.readlines()]\n",
    "\n",
    "print(webs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path_data, \"PoetryData.csv\"))\n",
    "df = extract_webs(webs, df)\n",
    "# df = extract_webs(webs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(path_data, \"PoetryData.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding bad apples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra = extract_text(os.path.join(path_data, \"bad_apples.txt\"))\n",
    "df = pd.concat([df, df_extra], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(path_data, \"PoetryData.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylistic features classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Poem', 'Poet'], dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(path_data, \"PoetryData.csv\"))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Poet\n",
       "Christina Rossetti     30\n",
       "E. E. Cummings         30\n",
       "Emily Dickinson        30\n",
       "John Keats             30\n",
       "Langston Hughes        30\n",
       "Naomi Shihab Nye       30\n",
       "Robert Frost           30\n",
       "W. B. Yeats            30\n",
       "Walt Whitman           30\n",
       "William Shakespeare    30\n",
       "Name: Title, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Poet'])['Title'].count().nlargest(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "df = df.drop_duplicates(subset=['Title'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Poet\n",
       "Christina Rossetti     30\n",
       "E. E. Cummings         30\n",
       "Emily Dickinson        30\n",
       "John Keats             30\n",
       "Langston Hughes        30\n",
       "Naomi Shihab Nye       30\n",
       "Robert Frost           30\n",
       "W. B. Yeats            30\n",
       "Walt Whitman           30\n",
       "William Shakespeare    30\n",
       "Name: Title, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Poet'])['Title'].count().nlargest(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['Poet']).head(30).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "df.loc[:, \"Poem\"] = df.loc[:, \"Poem\"].str.replace(\"\\r\",\"\")\n",
    "df.loc[:, \"Poem\"] = df.loc[:, \"Poem\"].str.rstrip(\"\\n\")\n",
    "\n",
    "df.loc[:, \"Title\"] = df.loc[:, \"Title\"].str.replace(r\" {2,}\", \"\", regex=True)\n",
    "df.loc[:, \"Title\"] = df.loc[:, \"Title\"].str.replace(\"\\n\", \"\")\n",
    "df.loc[:, \"Title\"] = df.loc[:, \"Title\"].str.replace(\"\\r\", \"\")\n",
    "\n",
    "df.loc[:, \"Poet\"] = df.loc[:, \"Poet\"].str.replace(\"\\n\", \"\")\n",
    "df.loc[:, \"Poet\"] = df.loc[:, \"Poet\"].str.replace(\"\\r\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num NaN Poems: 0\n",
      "Num NaN Poets: 0\n",
      "Num NaN Title: 0\n"
     ]
    }
   ],
   "source": [
    "nan_poems = df[\"Poem\"].isnull().sum()\n",
    "nan_poets = df[\"Poet\"].isnull().sum()\n",
    "nan_title = df[\"Title\"].isnull().sum()\n",
    "print(f\"Num NaN Poems: {nan_poems}\" +\n",
    "      f\"\\nNum NaN Poets: {nan_poets}\" +\n",
    "      f\"\\nNum NaN Title: {nan_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Poem text files should be formatted as:\n",
    "TITLE\n",
    "AUTHOR\n",
    "TEXTTEXTTEXT[...]\n",
    "******\n",
    "TITLE2\n",
    "AUTHOR(2)\n",
    "TEXT...\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(path_data, \"style_input.txt\"), \"w\", encoding='UTF-8') as f:\n",
    "    aux = \"\"\n",
    "    for _, row in df.iterrows():\n",
    "        new_line = str(row[\"Title\"]) + \"\\n\" + str(row[\"Poet\"]) + \"\\n\" + str(row[\"Poem\"]) + \"\\n******\\n\"\n",
    "        \n",
    "        f.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open output file, remove first file and also the separators\n",
    "with open(os.path.join(path_data, \"out.txt\"), \"r\", encoding='UTF-8', errors='ignore') as f :\n",
    "    output = f.readlines()\n",
    "output = output[1:]\n",
    "output = list(filter(lambda a: \"***\" not in a, output))\n",
    "\n",
    "#Split for each line and store in a list (probably useless but idc)\n",
    "out_list = []\n",
    "for elem in output :\n",
    "    content = elem.split(\"|\")[2:]\n",
    "    #remove \\n of last e\n",
    "    content[-1] = content[-1][:-1]\n",
    "    \n",
    "    out_list.append(content)\n",
    "\n",
    "style_ds = np.array(out_list, dtype=np.float64)\n",
    "\n",
    "with open(os.path.join(path_data, \"out.npy\"), 'wb') as f:\n",
    "    np.save(f, style_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 84)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(path_data, \"out.npy\"), 'rb') as f:\n",
    "    style_ds = np.load(f)\n",
    "\n",
    "style_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poems generated with GTP-J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Poet', 'Poem', 'Title'], dtype='object')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_fake = extract_text(os.path.join(path_data, \"fake_input.txt\"))\n",
    "df_fake = pd.read_csv(os.path.join(path_data, \"PoetryData_fake.csv\"))\n",
    "df_fake.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Poet\n",
       "Christina Rossetti     10\n",
       "E. E. Cummings         10\n",
       "Emily Dickinson        10\n",
       "John Keats             10\n",
       "Langston Hughes        10\n",
       "Naomi Shihab Nye       10\n",
       "Robert Frost           10\n",
       "W. B. Yeats            10\n",
       "Walt Whitman           10\n",
       "William Shakespeare    10\n",
       "Name: Title, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake = df_fake.dropna()\n",
    "df_fake = df_fake.drop_duplicates(subset=['Title'], ignore_index=True)\n",
    "df_fake.to_csv(os.path.join(path_data, \"PoetryData_fake.csv\"),index=False)\n",
    "df_fake.groupby(['Poet'])['Title'].count().nlargest(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num NaN Poems: 0\n",
      "Num NaN Poets: 0\n",
      "Num NaN Title: 0\n"
     ]
    }
   ],
   "source": [
    "# cleaning\n",
    "df_fake.loc[:, \"Poem\"] = df_fake.loc[:, \"Poem\"].str.replace(\"\\r\",\"\")\n",
    "df_fake.loc[:, \"Poem\"] = df_fake.loc[:, \"Poem\"].str.rstrip(\"\\n\")\n",
    "\n",
    "df_fake.loc[:, \"Title\"] = df_fake.loc[:, \"Title\"].str.replace(r\" {2,}\", \"\", regex=True)\n",
    "df_fake.loc[:, \"Title\"] = df_fake.loc[:, \"Title\"].str.replace(\"\\n\", \"\")\n",
    "df_fake.loc[:, \"Title\"] = df_fake.loc[:, \"Title\"].str.replace(\"\\r\", \"\")\n",
    "\n",
    "df_fake.loc[:, \"Poet\"] = df_fake.loc[:, \"Poet\"].str.replace(\"\\n\", \"\")\n",
    "df_fake.loc[:, \"Poet\"] = df_fake.loc[:, \"Poet\"].str.replace(\"\\r\", \"\")\n",
    "\n",
    "nan_poems = df_fake[\"Poem\"].isnull().sum()\n",
    "nan_poets = df_fake[\"Poet\"].isnull().sum()\n",
    "nan_title = df_fake[\"Title\"].isnull().sum()\n",
    "print(f\"Num NaN Poems: {nan_poems}\" +\n",
    "      f\"\\nNum NaN Poets: {nan_poets}\" +\n",
    "      f\"\\nNum NaN Title: {nan_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Poem text files should be formatted as:\n",
    "TITLE\n",
    "AUTHOR\n",
    "TEXTTEXTTEXT[...]\n",
    "******\n",
    "TITLE2\n",
    "AUTHOR(2)\n",
    "TEXT...\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(path_data, \"fake_input.txt\"), \"w\", encoding='UTF-8') as f:\n",
    "    aux = \"\"\n",
    "    for _, row in df_fake.iterrows():\n",
    "        new_line = str(row[\"Title\"]) + \"\\n\" + str(row[\"Poet\"]) + \"\\n\" + str(row[\"Poem\"]) + \"\\n******\\n\"\n",
    "        \n",
    "        f.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open output file, remove first file and also the separators\n",
    "with open(os.path.join(path_data, \"fake_out.txt\"), \"r\", encoding='UTF-8', errors='ignore') as f :\n",
    "    fake_output = f.readlines()\n",
    "fake_output = fake_output[1:]\n",
    "fake_output = list(filter(lambda a: \"***\" not in a, fake_output))\n",
    "\n",
    "#Split for each line and store in a list (probably useless but idc)\n",
    "fake_out_list = []\n",
    "for elem in fake_output :\n",
    "    content = elem.split(\"|\")[2:]\n",
    "    #remove \\n of last e\n",
    "    content[-1] = content[-1][:-1]\n",
    "    \n",
    "    fake_out_list.append(content)\n",
    "\n",
    "style_fake_ds = np.array(fake_out_list, dtype=np.float64)\n",
    "\n",
    "with open(os.path.join(path_data, \"fake_out.npy\"), 'wb') as f:\n",
    "    np.save(f, style_fake_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 84)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(path_data, \"fake_out.npy\"), 'rb') as f:\n",
    "    style_fake_ds = np.load(f)\n",
    "\n",
    "style_fake_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the points clouds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"distance = sr.Distance(spatial.distance.pdist(data, \"euclidean\"))\n",
    "wisconsin_distances = distance.square_form()\n",
    "i = 0\n",
    "while i < len(wisconsin_distances):\n",
    "    plt.hist(wisconsin_distances[i])\n",
    "    i += 1\n",
    "\n",
    "distributions = {}\n",
    "distributions[\"0_15\"] = sr.get_distribution(name=\"uniform\", interval=[0,15])\n",
    "distributions[\"5_20\"] = sr.get_distribution(name=\"uniform\", interval=[5,20])\n",
    "distributions[\"10_25\"] = sr.get_distribution(name=\"uniform\", interval=[10,25])\n",
    "distributions[\"10_30\"] = sr.get_distribution(name=\"uniform\", interval=[10,30])\n",
    "distributions[\"15_30\"] = sr.get_distribution(name=\"uniform\", interval=[15,30])\n",
    "\n",
    "probabilities = {}\n",
    "for k in distributions.keys():\n",
    "    probabilities[k] = distributions[k](wisconsin_distances)\n",
    "\n",
    "number_instances=300\n",
    "sample_size=30\n",
    "\n",
    "start = timer()    \n",
    "h0_sr = {}\n",
    "h1_sr = {}\n",
    "for k in  distributions.keys():\n",
    "    h0_sr[k] = []\n",
    "    h1_sr[k] = []\n",
    "    for patient in wisconsin_distances:\n",
    "        p = distributions[k](patient)\n",
    "        s = sr.get_sample(number_instances, sample_size, p)\n",
    "        f = distance.get_h0sr(sample=s,clustering_method=\"complete\")\n",
    "        b = distance.get_bc(sample=s, maxdim=1)\n",
    "        g = sr.bc_to_sr(b,degree=\"H1\")\n",
    "        h0_sr[k].append(f)\n",
    "        h1_sr[k].append(g)\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))        \n",
    "\n",
    "for k in distributions.keys():\n",
    "    fig = plt.figure(k,figsize=(30,30))\n",
    "    i = 0\n",
    "    for f in h0_sr[k]:\n",
    "        if classification[i] ==2:\n",
    "            color = \"black\"\n",
    "        else:\n",
    "            color = \"red\"\n",
    "        f.plot(color = color)\n",
    "        i += 1\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation in pipeline_from_csv_to_bert.ipynb\n",
    "# Extraction from save file\n",
    "\n",
    "with open(os.path.join(path_data, \"bert.npy\"), 'rb') as f:\n",
    "    sem_ds = np.load(f)\n",
    "\n",
    "sem_ds.shape\n",
    "\n",
    "\n",
    "with open(os.path.join(path_data, \"fake_bert.npy\"), 'rb') as f:\n",
    "    fake_sem_ds = np.load(f)\n",
    "\n",
    "fake_sem_ds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26105b2dc5f82e64c8b7ee8f072b1281e0bdab5f382708ceed93e75530eb79f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
