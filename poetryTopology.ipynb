{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import time, random\n",
    "\n",
    "\n",
    "import stablerank.srank as sr\n",
    "from ripser import ripser\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import scipy.spatial as spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"./Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_poem(driver):\n",
    "    elem = driver.find_element(By.CLASS_NAME, \"card-body\")\n",
    "    return \"\\n\\n\".join(p.text for p in elem.find_elements(By.CSS_SELECTOR, \"p\"))\n",
    "\n",
    "def iterate_poems(driver):\n",
    "\n",
    "    def get_elements():\n",
    "        mytable = driver.find_element(By.CSS_SELECTOR, 'tbody')\n",
    "        return mytable.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "    poems = []\n",
    "    titles = []\n",
    "    auth_elem = driver.find_element(By.CLASS_NAME, \"poet__name\")\n",
    "    author = auth_elem.text\n",
    "    main_window = driver.current_window_handle\n",
    "\n",
    "    i = 0\n",
    "    elements = get_elements()\n",
    "    for link in elements:\n",
    "        titles.append(link.text)\n",
    "\n",
    "        # Open link in new tab\n",
    "        link.send_keys(Keys.CONTROL + Keys.RETURN)\n",
    "        windows = driver.window_handles\n",
    "        driver.switch_to.window(windows[-1])\n",
    "\n",
    "        # Extract poem\n",
    "        time.sleep(2)\n",
    "        poems.append(extract_poem(driver))\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Close Current Tab\n",
    "        driver.close()\n",
    "\n",
    "        # Put focus back on main window\n",
    "        driver.switch_to.window(main_window)\n",
    "        time.sleep(2)\n",
    "\n",
    "    return author, titles, poems\n",
    "\n",
    "def iterate_web(driver, web, df=None):\n",
    "    driver.get(web)\n",
    "    time.sleep(1)\n",
    "\n",
    "    more_next = True\n",
    "    if df is None:\n",
    "        df = pd.DataFrame({\"Poet\":[], \"Poem\":[], \"Title\":[]})\n",
    "\n",
    "    while more_next:\n",
    "        # Obtain all poems author\n",
    "        author, titles, poems = iterate_poems(driver)\n",
    "\n",
    "        df_aux = pd.DataFrame({\"Poet\":[author]*len(poems),\n",
    "                                \"Poem\":poems,\n",
    "                                \"Title\":titles})\n",
    "\n",
    "        df = pd.concat([df, df_aux], ignore_index=True)\n",
    "        del df_aux\n",
    "\n",
    "        try:\n",
    "            # Remove spam covering next\n",
    "            link = driver.find_element(By.XPATH,\n",
    "                                        '/html/body/w-div/span')\n",
    "            link.click()\n",
    "            time.sleep(0.5)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        try:     \n",
    "            # Click next   \n",
    "            link = driver.find_element(By.CSS_SELECTOR,\n",
    "                                        '[aria-label=\"Go to next page\"]')\n",
    "            link.click()\n",
    "        except NoSuchElementException:\n",
    "            more_next = False\n",
    "        \n",
    "        \n",
    "    return df\n",
    "\n",
    "def extract_webs(webs):\n",
    "    driver = webdriver.Chrome()\n",
    "    df = pd.DataFrame({\"Poet\":[], \"Poem\":[], \"Title\":[]})\n",
    "    for web in webs:\n",
    "        df = iterate_web(driver, web, df)\n",
    "    \n",
    "    #time.sleep(50)\n",
    "    driver.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "webs=[]\n",
    "with open(os.path.join(path_data, \"poets.txt\"), \"r\") as f:\n",
    "        webs.append(f.readline())\n",
    "        \n",
    "df = extract_webs(webs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(path_data, \"PoetryData.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylistic features classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Poet', 'Poem', 'Title'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(path_data, \"PoetryData.csv\"))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red = df[[\"Title\", \"Poem\", \"Poet\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_red.groupby(['Poet'])['Poet'].count().nlargest(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Poet\n",
       "John Wieners                17\n",
       "Cristin O'Keefe Aptowicz     8\n",
       "Name: Poet, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "df_red.loc[:, \"Poem\"] = df_red.loc[:, \"Poem\"].str.replace(\"\\r\",\"\")\n",
    "df_red.loc[:, \"Poem\"] = df_red.loc[:, \"Poem\"].str.rstrip(\"\\n\")\n",
    "\n",
    "df_red.loc[:, \"Title\"] = df_red.loc[:, \"Title\"].str.replace(r\" {2,}\", \"\", regex=True)\n",
    "df_red.loc[:, \"Title\"] = df_red.loc[:, \"Title\"].str.replace(\"\\n\", \"\")\n",
    "df_red.loc[:, \"Title\"] = df_red.loc[:, \"Title\"].str.replace(\"\\r\", \"\")\n",
    "\n",
    "df_red.loc[:, \"Poet\"] = df_red.loc[:, \"Poet\"].str.replace(\"\\n\", \"\")\n",
    "df_red.loc[:, \"Poet\"] = df_red.loc[:, \"Poet\"].str.replace(\"\\r\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num NaN Poems: 0\n",
      "Num NaN Poets: 0\n",
      "Num NaN Title: 0\n"
     ]
    }
   ],
   "source": [
    "nan_poems = df_red[\"Poem\"].isnull().sum()\n",
    "nan_poets = df_red[\"Poet\"].isnull().sum()\n",
    "nan_title = df_red[\"Title\"].isnull().sum()\n",
    "print(f\"Num NaN Poems: {nan_poems}\" +\n",
    "      f\"\\nNum NaN Poets: {nan_poets}\" +\n",
    "      f\"\\nNum NaN Title: {nan_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Poem text files should be formatted as:\n",
    "TITLE\n",
    "AUTHOR\n",
    "TEXTTEXTTEXT[...]\n",
    "******\n",
    "TITLE2\n",
    "AUTHOR(2)\n",
    "TEXT...\n",
    "\"\"\"\n",
    "\n",
    "with codecs.open(os.path.join(path_data, \"style_input.txt\"), \"w\", \"utf-8\") as f:\n",
    "    aux = \"\"\n",
    "    for _, row in df_red.iterrows():\n",
    "        new_line = str(row[\"Title\"]) + \"\\n\" + str(row[\"Poet\"]) + \"\\n\" + str(row[\"Poem\"]) + \"\\n******\\n\"\n",
    "        \n",
    "        f.write(new_line)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the points clouds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = sr.Distance(spatial.distance.pdist(data, \"euclidean\"))\n",
    "wisconsin_distances = distance.square_form()\n",
    "i = 0\n",
    "while i < len(wisconsin_distances):\n",
    "    plt.hist(wisconsin_distances[i])\n",
    "    i += 1\n",
    "\n",
    "distributions = {}\n",
    "distributions[\"0_15\"] = sr.get_distribution(name=\"uniform\", interval=[0,15])\n",
    "distributions[\"5_20\"] = sr.get_distribution(name=\"uniform\", interval=[5,20])\n",
    "distributions[\"10_25\"] = sr.get_distribution(name=\"uniform\", interval=[10,25])\n",
    "distributions[\"10_30\"] = sr.get_distribution(name=\"uniform\", interval=[10,30])\n",
    "distributions[\"15_30\"] = sr.get_distribution(name=\"uniform\", interval=[15,30])\n",
    "\n",
    "probabilities = {}\n",
    "for k in distributions.keys():\n",
    "    probabilities[k] = distributions[k](wisconsin_distances)\n",
    "\n",
    "number_instances=300\n",
    "sample_size=30\n",
    "\n",
    "start = timer()    \n",
    "h0_sr = {}\n",
    "h1_sr = {}\n",
    "for k in  distributions.keys():\n",
    "    h0_sr[k] = []\n",
    "    h1_sr[k] = []\n",
    "    for patient in wisconsin_distances:\n",
    "        p = distributions[k](patient)\n",
    "        s = sr.get_sample(number_instances, sample_size, p)\n",
    "        f = distance.get_h0sr(sample=s,clustering_method=\"complete\")\n",
    "        b = distance.get_bc(sample=s, maxdim=1)\n",
    "        g = sr.bc_to_sr(b,degree=\"H1\")\n",
    "        h0_sr[k].append(f)\n",
    "        h1_sr[k].append(g)\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))        \n",
    "\n",
    "for k in distributions.keys():\n",
    "    fig = plt.figure(k,figsize=(30,30))\n",
    "    i = 0\n",
    "    for f in h0_sr[k]:\n",
    "        if classification[i] ==2:\n",
    "            color = \"black\"\n",
    "        else:\n",
    "            color = \"red\"\n",
    "        f.plot(color = color)\n",
    "        i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95c1efeb9379afe4535166b7dabdd8cbd79c9c83966898d9af486774d6e7ab86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
