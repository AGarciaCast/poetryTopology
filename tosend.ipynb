{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full info classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sem = np.array([poem.mean(axis=0) for poem in sem_ds])\n",
    "print(mean_sem.shape)\n",
    "full_ds = np.hstack([style_ds, mean_sem])\n",
    "print(full_ds.shape)\n",
    "\n",
    "with open(os.path.join(path_data, \"full_save.npy\"), 'wb') as f:\n",
    "    np.save(f, full_ds)\n",
    "\n",
    "\n",
    "mean_fake_sem = np.array([poem.mean(axis=0) for poem in fake_sem_ds])\n",
    "full_fake_ds = np.hstack([style_fake_ds, mean_fake_sem])\n",
    "print(full_fake_ds.shape)\n",
    "\n",
    "with open(os.path.join(path_data, \"full_fake_save.npy\"), 'wb') as f:\n",
    "    np.save(f, full_fake_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data, \"full_save.npy\"), 'rb') as f:\n",
    "    full_ds = np.load(f)\n",
    "\n",
    "with open(os.path.join(path_data, \"full_fake_save.npy\"), 'rb') as f:\n",
    "    full_ds_fake = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the same test/Train than before\n",
    "full_ds_train=np.delete(full_ds,list_test,0)\n",
    "full_ds_test=np.delete(full_ds,list_train,0)\n",
    "\n",
    "print(full_ds_train.shape)\n",
    "print(full_ds_test.shape)\n",
    "\n",
    "full_ds_mix = []\n",
    "for i in range(10):\n",
    "    full_ds_mix.append(full_ds_train[25*i:25*(i+1), :])\n",
    "    full_ds_mix.append(full_ds_fake[10*i:10*(i+1), :])\n",
    "\n",
    "full_ds_mix = np.vstack(full_ds_mix)\n",
    "print(full_ds_mix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA(n_components = 50, random_state = 6)\n",
    "train_pca = pca_full.fit_transform(full_ds_train)\n",
    "tsne_full = TSNE(n_components = 2, perplexity = 10, learning_rate=\"auto\", init=\"pca\", random_state=6)\n",
    "train_tsne = tsne_full.fit_transform(train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pca = pca_full.fit_transform(full_ds_test)\n",
    "test_tsne = tsne_full.fit_transform(test_pca)\n",
    "fake_pca = pca_full.fit_transform(full_ds_fake)\n",
    "fake_tsne = tsne_full.fit_transform(fake_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_poems_train = 250\n",
    "nb_poems_test= 50\n",
    "nb_poems_fake =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,5))\n",
    "plt.scatter(train_tsne[:,0],train_tsne[:,1],c=color_train, s=10, cmap=\"rainbow\", label=\"Train set\")\n",
    "plt.scatter(test_tsne[:,0],test_tsne[:,1], c=color_test,s=50, marker=\"^\",cmap=\"rainbow\", label=\"Test set\")\n",
    "plt.scatter(fake_tsne[:,0],fake_tsne[:,1], c=color_fake,s=100, marker=\"+\",cmap=\"rainbow\",label=\"Fake set\")\n",
    "plt.title(\"Projection with t-SNE of the full data set after standardisation\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler_full = preprocessing.StandardScaler().fit(full_ds_train)\n",
    "full_ds_train = scaler_full.transform(full_ds_train)\n",
    "full_ds_test = scaler_full.transform(full_ds_test)\n",
    "full_ds_fake = scaler_full.transform(full_ds_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 1, figsize=(10,5))\n",
    "ax[0].boxplot(full_ds_train[:,:])\n",
    "ax[1].boxplot(full_ds_fake[:,:])\n",
    "ax[2].boxplot(full_ds_test[:,:])\n",
    "ax[3].boxplot(full_ds_fake[:,:])\n",
    "ax[4].boxplot(full_ds_train[:,:])\n",
    "ax[5].boxplot(full_ds_test[:,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(10,5))\n",
    "ax[0].boxplot(full_ds_fake[:,0:100])\n",
    "ax[1].boxplot(full_ds_train[:,0:100])\n",
    "ax[2].boxplot(full_ds_test[:,0:100]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_full=np.mean(full_ds_train,0)  #(de chauque compo)\n",
    "std_full=np.std(full_ds_train,0)\n",
    "\n",
    "\n",
    "print(mean_full)\n",
    "print(std_full[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_full=np.mean(full_ds_fake,0)  #(de chauque compo)\n",
    "std_full=np.std(full_ds_fake,0)\n",
    "\n",
    "print(mean_full)\n",
    "print(std_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist_full = \"euclidean\"\n",
    "#dist_full = \"cityblock\"\n",
    "\n",
    "def adapt_dist(vec1,vec2):\n",
    "    n = len(vec1)\n",
    "    part_feat=spatial.distance.cityblock(vec1[:83],vec2[:83])\n",
    "    part_emb=spatial.distance.euclidean(vec1[84:1023],vec2[84:1023])\n",
    "    return part_feat+part_emb\n",
    "\n",
    "#dist_full = adapt_dist\n",
    "dist_full =\"adapt_non_scale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_full = spatial.distance.pdist(full_ds_train, dist_full)\n",
    "distance_train_full = sr.Distance(d_train_full)\n",
    "matrix_distance_train_full = distance_train_full.square_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.xticks(range(0, int(np.max(matrix_distance_fake_full)), 1))\n",
    "plt.xlabel(\"Pairwise distance\")\n",
    "plt.ylabel(\"Absolute freq.\")\n",
    "plt.title(\"Full distance histogram (all authors)\")\n",
    "plt.hist(matrix_distance_train_full)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram by author \n",
    "fig, ax = plt.subplots(2, 5, figsize=(20,5), sharey=True)\n",
    "\n",
    "for i, txt in enumerate(list_author): \n",
    "    ax[i//5, i%5].hist(matrix_distance_train_full[25*i:25*(i+1), :])\n",
    "    ax[i//5, i%5].set_title(txt)\n",
    "\n",
    "fig.suptitle(\"Full distance histogram per author \", fontsize=15)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.82)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions_full = {\n",
    "    \"euclidean\":{\n",
    "        \"0_38\": sr.get_distribution(name=\"uniform\", interval=[0,38]),\n",
    "        \"0_48\": sr.get_distribution(name=\"uniform\", interval=[0,48]),\n",
    "        \"0_57\": sr.get_distribution(name=\"uniform\", interval=[0,57]),\n",
    "        # if you consider closest points as the same --> Ring neightbourhood\n",
    "        \"38_57\": sr.get_distribution(name=\"uniform\", interval=[38,57]),\n",
    "        \"48_105\": sr.get_distribution(name=\"uniform\", interval=[48,105]),\n",
    "        },\n",
    "    \"cityblock\":{\n",
    "        \"0_1000\": sr.get_distribution(name=\"uniform\", interval=[0,1000]),\n",
    "        \"0_1300\": sr.get_distribution(name=\"uniform\", interval=[0,1300]),\n",
    "        \"0_1550\": sr.get_distribution(name=\"uniform\", interval=[0,1550]),\n",
    "        # if you consider closest points as the same --> Ring neightbourhood\n",
    "        \"1300_2500\": sr.get_distribution(name=\"uniform\", interval=[1300,2500]),\n",
    "        \"1000_1800\": sr.get_distribution(name=\"uniform\", interval=[1000,1800]),\n",
    "        },\n",
    "    \"adapt_dist\":{\n",
    "        \"0_105\": sr.get_distribution(name=\"uniform\", interval=[0,105]),\n",
    "        \"0_126\": sr.get_distribution(name=\"uniform\", interval=[0,126]),\n",
    "        \"0_150\": sr.get_distribution(name=\"uniform\", interval=[0,150]),\n",
    "        # if you consider closest points as the same --> Ring neightbourhood\n",
    "        \"105_126\": sr.get_distribution(name=\"uniform\", interval=[105,126]),\n",
    "        \"105_210\": sr.get_distribution(name=\"uniform\", interval=[105,210]),\n",
    "        },\n",
    "    \"adapt_non_scale\":{\n",
    "        \"0_7.7\": sr.get_distribution(name=\"uniform\", interval=[0,7.7]),\n",
    "        \"0_9.1\": sr.get_distribution(name=\"uniform\", interval=[0,9.1]),\n",
    "        \"0_10.8\": sr.get_distribution(name=\"uniform\", interval=[0,10.8]),\n",
    "        # if you consider closest points as the same --> Ring neightbourhood\n",
    "        \"6_10.8\": sr.get_distribution(name=\"uniform\", interval=[6,10.8]),\n",
    "        \"7.7_15\": sr.get_distribution(name=\"uniform\", interval=[7.7,15]),\n",
    "        }\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test_full = spatial.distance.pdist(full_ds_test, dist_full)\n",
    "distance_test_full = sr.Distance(d_test_full)\n",
    "matrix_distance_test_full = distance_test_full.square_form()\n",
    "\n",
    "d_fake_full = spatial.distance.pdist(full_ds_fake, dist_full)\n",
    "distance_fake_full = sr.Distance(d_fake_full)\n",
    "matrix_distance_fake_full = distance_fake_full.square_form()\n",
    "\n",
    "\n",
    "d_mix_full = spatial.distance.pdist(full_ds_mix, dist_full)\n",
    "distance_mix_full = sr.Distance(d_mix_full)\n",
    "matrix_distance_mix_full = distance_mix_full.square_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.xticks(range(0, int(np.max(matrix_distance_fake_full)), 5))\n",
    "plt.xlabel(\"Pairwise distance\")\n",
    "plt.ylabel(\"Absolute freq.\")\n",
    "plt.title(\"Full distance histogram (all authors)\")\n",
    "plt.hist(matrix_distance_fake_full)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the stable ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "number_instances = 300\n",
    "sample_size = 30\n",
    "  \n",
    "\n",
    "links = [\"single\", \"complete\", \"average\", \"ward\"]\n",
    "\n",
    "# dict{defaultdict{dict{dict{list}}}}\n",
    "logs_full = {\n",
    "    \"train\": defaultdict(lambda: {}),\n",
    "    \"mix\": defaultdict(lambda: {}),\n",
    "    \"test\": defaultdict(lambda: {}),\n",
    "    \"fake\": defaultdict(lambda: {})\n",
    "\n",
    "}\n",
    "\n",
    "matrix_distance_all_full = {\n",
    "    \"train\": matrix_distance_train_full,\n",
    "    \"mix\": matrix_distance_mix_full,\n",
    "    \"test\": matrix_distance_test_full,\n",
    "    \"fake\": matrix_distance_fake_full\n",
    "    \n",
    "}\n",
    "\n",
    "distance_obj_all_full = {\n",
    "    \"train\": distance_train_full,\n",
    "    \"mix\": distance_mix_full,\n",
    "    \"test\": distance_test_full,\n",
    "    \"fake\": distance_fake_full\n",
    "    \n",
    "}\n",
    "\n",
    "for dataset, matrix_distance in matrix_distance_all_full.items():\n",
    "    h1_computed = False\n",
    "    for link in links:\n",
    "        h0_sr = {}\n",
    "        h1_sr = {}\n",
    "        bc = {}\n",
    "        for k in  distributions_full[dist_full].keys():\n",
    "            h0_sr[k] = []\n",
    "            h1_sr[k] = []\n",
    "            bc[k] = []\n",
    "            for poem in matrix_distance:\n",
    "                # Sampling in interval\n",
    "                p = distributions_full[dist_full][k](poem)\n",
    "                s = sr.get_sample(number_instances, sample_size, p)\n",
    "                \n",
    "                # H0 stable rank, all samples\n",
    "                f = distance_obj_all_full[dataset].get_h0sr(sample=s, clustering_method=link)\n",
    "                \n",
    "                # Bar code, one sample\n",
    "                s_1 = sr.get_sample(1, sample_size, p)\n",
    "                b_1 = distance_obj_all_full[dataset].get_bc(sample=s_1, maxdim=1, reduced=False)\n",
    "                \n",
    "                if not h1_computed:\n",
    "                    # H1 bar code and stable rank, all samples \n",
    "                    b = distance_obj_all_full[dataset].get_bc(sample=s, maxdim=1)\n",
    "                    g = sr.bc_to_sr(b, degree=\"H1\")\n",
    "                    h1_sr[k].append(g)\n",
    "\n",
    "                h0_sr[k].append(f)\n",
    "                bc[k].append(b_1)\n",
    "        \n",
    "        logs_full[dataset][link][\"h0\"] = h0_sr\n",
    "        logs_full[dataset][link][\"h1\"] = h1_sr if not h1_computed else logs_full[dataset][links[0]][\"h1\"]\n",
    "        h1_computed=True\n",
    "        logs_full[dataset][link][\"bc\"] = bc\n",
    "\n",
    "with open(os.path.join(path_data, f\"logs_full_{dist_full}.p\"), \"wb\") as f:\n",
    "    pickle.dump(logs_full,  f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_data,f\"logs_full_{dist_full}.p\"), \"rb\" ) as f:\n",
    "  logs_full =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_full(logs, mode=\"all_plots\", nb_poems=nb_poems, data_name=\"train\"):\n",
    "    list_integers = range(0,10) #the range and variance of this list will vary \n",
    "    cmap = cm.rainbow(np.array(list_integers)/(2*np.mean(list_integers)))\n",
    "    custom_lines = [Line2D([0], [0], color=cmap[i], lw=2) for i in range(10)]\n",
    "    num_poems_x_auth = nb_poems//10\n",
    "    \n",
    "    plotted = False\n",
    "    num_splits = len(distributions_full[dist_full].keys())\n",
    "    \n",
    "    if mode.lower() == \"all_auth\" or mode.lower() == \"all_plots\":\n",
    "        #h0 & h1 plot all authors\n",
    "        links = logs.keys()\n",
    "        fig, ax = plt.subplots(len(links) + 1, num_splits, figsize=(5*num_splits, 5*(len(links) + 1)))\n",
    "        for link_idx, link in enumerate(links):\n",
    "            h0_sr = logs[link][\"h0\"] \n",
    "            h1_sr = logs[link][\"h1\"] \n",
    "            \n",
    "            for idx, key in enumerate(distributions_full[dist_full].keys()):\n",
    "                i = 0\n",
    "                color = -1\n",
    "                for h0, h1 in zip(h0_sr[key], h1_sr[key]):\n",
    "                    if i % num_poems_x_auth == 0:\n",
    "                        color += 1\n",
    "                        \n",
    "                    h0.plot(color=cmap[color], ax=ax[link_idx, idx])\n",
    "                    \n",
    "                    if link_idx == 0:\n",
    "                        h1.plot(color=cmap[color], ax=ax[-1, idx])\n",
    "\n",
    "                    i += 1\n",
    "                    \n",
    "                if link_idx == 0:\n",
    "                    ax[0, idx].set_title(f\"Split {key}\")\n",
    "            \n",
    "            ax[link_idx, 0].set_ylabel(r\"$H_0$ mean $\\widehat{rank}$\" + f\" ({link})\")\n",
    "            \n",
    "        ax[-1, 0].set_ylabel(r\"$H_1$ mean $\\widehat{rank}$\")\n",
    "\n",
    "        fig.legend(custom_lines, [s for s in list_author],\n",
    "                    ncol=2)\n",
    "\n",
    "        fig.suptitle(\"Full stable rank per split\", fontsize=20, y=0.96)\n",
    "        fig.tight_layout()\n",
    "        fig.subplots_adjust(top=0.92)\n",
    "\n",
    "        # Get the bounding boxes of the axes including text decorations\n",
    "        r = fig.canvas.get_renderer()\n",
    "        get_bbox = lambda ax: ax.get_tightbbox(r).transformed(fig.transFigure.inverted())\n",
    "        bboxes = np.array(list(map(get_bbox, ax.flat)), mtrans.Bbox).reshape(ax.shape)\n",
    "\n",
    "        #Get the minimum and maximum extent, get the coordinate half-way between those\n",
    "        ymax = np.array(list(map(lambda b: b.y1, bboxes.flat))).reshape(ax.shape).max(axis=1)\n",
    "        ymin = np.array(list(map(lambda b: b.y0, bboxes.flat))).reshape(ax.shape).min(axis=1)\n",
    "        ys = np.c_[ymax[1:], ymin[:-1]].mean(axis=1)\n",
    "\n",
    "        # Draw a horizontal lines at those coordinates\n",
    "        y = ys[-1]\n",
    "        line = plt.Line2D([0,1],[y,y], transform=fig.transFigure, color=\"grey\")\n",
    "        fig.add_artist(line)\n",
    "        plt.savefig(os.path.join(path_out, f\"Figures/Full/{dist_full.capitalize()}/full_{data_name}_sr_all_{dist_full}.png\"), dpi=100)\n",
    "        plt.show()\n",
    "        plotted = True\n",
    "    \n",
    "    if mode.lower() == \"all_means\" or mode.lower() == \"all_plots\":\n",
    "        #h0 & h1 plot all authors\n",
    "        links = logs.keys()\n",
    "        fig, ax = plt.subplots(len(links) + 1, num_splits, figsize=(5*num_splits, 5*(len(links) + 1)))\n",
    "        for link_idx, link in enumerate(links):\n",
    "            h0_sr = logs[link][\"h0\"] \n",
    "            h1_sr = logs[link][\"h1\"] \n",
    "            for idx, key in enumerate(distributions_full[dist_full].keys()):\n",
    "                auth_logs_h0 =  [[] for _ in range(10)]\n",
    "                auth_logs_h1 =  [[] for _ in range(10)]\n",
    "                i = 0\n",
    "                color = -1\n",
    "                for h0, h1 in zip(h0_sr[key], h1_sr[key]):\n",
    "                    if i % num_poems_x_auth == 0:\n",
    "                        color += 1\n",
    "                    \n",
    "                    auth_logs_h0[color].append(h0)\n",
    "                    auth_logs_h1[color].append(h1)\n",
    "                    \n",
    "                    i += 1\n",
    "                \n",
    "                auth_idx = 0\n",
    "                for h0, h1 in zip(auth_logs_h0,auth_logs_h1):\n",
    "                    h0_mean = sum(h0)/len(h0)\n",
    "                    h1_mean = sum(h1)/len(h1)\n",
    "                    \n",
    "                    h0_mean.plot(color=cmap[auth_idx], ax=ax[link_idx, idx])\n",
    "                    if link_idx == 0:\n",
    "                        h1_mean.plot(color=cmap[auth_idx], ax=ax[-1, idx])\n",
    "\n",
    "                    auth_idx += 1\n",
    "                \n",
    "                if link_idx == 0:\n",
    "                    ax[0, idx].set_title(f\"Split {key}\")\n",
    "                \n",
    "            ax[link_idx, 0].set_ylabel(r\"$H_0$ mean $\\widehat{rank}$\" + f\" ({link})\")\n",
    "\n",
    "\n",
    "        ax[-1, 0].set_ylabel(r\"$H_1$ mean $\\widehat{rank}$\")\n",
    "\n",
    "        fig.legend(custom_lines, [s for s in list_author],\n",
    "                    ncol=2)\n",
    "\n",
    "        fig.suptitle(f\"Full mean stable rank per split\", fontsize=20, y=0.96)\n",
    "        fig.tight_layout()\n",
    "        fig.subplots_adjust(top=0.92)\n",
    "\n",
    "        # Get the bounding boxes of the axes including text decorations\n",
    "        r = fig.canvas.get_renderer()\n",
    "        get_bbox = lambda ax: ax.get_tightbbox(r).transformed(fig.transFigure.inverted())\n",
    "        bboxes = np.array(list(map(get_bbox, ax.flat)), mtrans.Bbox).reshape(ax.shape)\n",
    "\n",
    "        #Get the minimum and maximum extent, get the coordinate half-way between those\n",
    "        ymax = np.array(list(map(lambda b: b.y1, bboxes.flat))).reshape(ax.shape).max(axis=1)\n",
    "        ymin = np.array(list(map(lambda b: b.y0, bboxes.flat))).reshape(ax.shape).min(axis=1)\n",
    "        ys = np.c_[ymax[1:], ymin[:-1]].mean(axis=1)\n",
    "\n",
    "        # Draw a horizontal lines at those coordinates\n",
    "        y = ys[-1]\n",
    "        line = plt.Line2D([0,1],[y,y], transform=fig.transFigure, color=\"grey\")\n",
    "        fig.add_artist(line)\n",
    "        plt.savefig(os.path.join(path_out, f\"Figures/Full/{dist_full.capitalize()}/full_{data_name}_sr_mean_{dist_full}.png\"), dpi=100)\n",
    "        plt.show()\n",
    "        plotted = True\n",
    "    \n",
    "    if mode.lower() == \"each_auth\" or mode.lower() == \"all_plots\":\n",
    "        #h0 & h1 plot per author \n",
    "        cmap = cm.rainbow(np.array(list_integers)/(2*np.mean(list_integers)))\n",
    "        color = -1\n",
    "        for auth_idx, auth in enumerate(list_author):\n",
    "            color += 1\n",
    "            fig, ax = plt.subplots(len(links) + 1, num_splits, figsize=(5*num_splits, 5*(len(links) + 1)))\n",
    "            for link_idx, link in enumerate(links):\n",
    "                h0_sr = logs[link][\"h0\"] \n",
    "                h1_sr = logs[link][\"h1\"] \n",
    "                for idx, key in enumerate(distributions_full[dist_full].keys()):\n",
    "                    h_0_auth = []\n",
    "                    h_1_auth = []\n",
    "                    i = 0\n",
    "                    for h0, h1 in zip(h0_sr[key], h1_sr[key]):\n",
    "                        if i//num_poems_x_auth == auth_idx:\n",
    "                            h_0_auth.append(h0)\n",
    "                            h_1_auth.append(h1)\n",
    "                        else:\n",
    "                            h0.plot(color=\"grey\", ax=ax[link_idx, idx], alpha=0.6)\n",
    "                            if link_idx == 0:\n",
    "                                h1.plot(color=\"grey\", ax=ax[-1, idx], alpha=0.6)\n",
    "\n",
    "                        i += 1\n",
    "                    \n",
    "                    for h0, h1 in zip(h_0_auth, h_1_auth): \n",
    "                        h0.plot(color=cmap[color], ax=ax[link_idx, idx],)\n",
    "                        if link_idx == 0:\n",
    "                            h1.plot(color=cmap[color], ax=ax[-1, idx])\n",
    "                            \n",
    "                    if link_idx == 0:\n",
    "                        ax[0, idx].set_title(f\"Split {key}\")\n",
    "                        \n",
    "                ax[link_idx, 0].set_ylabel(r\"$H_0$ mean $\\widehat{rank}$\" + f\" ({link})\")\n",
    "\n",
    "\n",
    "            ax[-1, 0].set_ylabel(r\"$H_1$ mean $\\widehat{rank}$\")\n",
    "\n",
    "            fig.suptitle(f\"Full stable rank per split: {auth}\", fontsize=20)\n",
    "            fig.tight_layout()\n",
    "            fig.subplots_adjust(top=0.92)\n",
    "\n",
    "            # Get the bounding boxes of the axes including text decorations\n",
    "            r = fig.canvas.get_renderer()\n",
    "            get_bbox = lambda ax: ax.get_tightbbox(r).transformed(fig.transFigure.inverted())\n",
    "            bboxes = np.array(list(map(get_bbox, ax.flat)), mtrans.Bbox).reshape(ax.shape)\n",
    "\n",
    "            #Get the minimum and maximum extent, get the coordinate half-way between those\n",
    "            ymax = np.array(list(map(lambda b: b.y1, bboxes.flat))).reshape(ax.shape).max(axis=1)\n",
    "            ymin = np.array(list(map(lambda b: b.y0, bboxes.flat))).reshape(ax.shape).min(axis=1)\n",
    "            ys = np.c_[ymax[1:], ymin[:-1]].mean(axis=1)\n",
    "\n",
    "            # Draw a horizontal lines at those coordinates\n",
    "            y = ys[-1]\n",
    "            line = plt.Line2D([0,1],[y,y], transform=fig.transFigure, color=\"grey\")\n",
    "            fig.add_artist(line)\n",
    "            auth_name = auth.replace(\" \", \"_\").replace(\".\", \"\")\n",
    "            plt.savefig(os.path.join(path_out,\n",
    "                        f\"Figures/Full/{dist_full.capitalize()}/full_{data_name}_sr_{auth_name}_{dist_full}.png\"),\n",
    "                        dpi=100)\n",
    "            plt.show()\n",
    "            plotted = True\n",
    "            \n",
    "    if not plotted:\n",
    "        print(f\"{mode} is not and option!\")\n",
    "        return -1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_full(logs_full[\"train\"], nb_poems=nb_poems);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_full(logs_full[\"fake\"], nb_poems=nb_poems_fake, data_name=\"fake\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\"single\", \"complete\", \"average\", \"ward\"]\n",
    "\n",
    "normal_train_res_full_mean = pd.DataFrame(columns=distributions_full[dist_full].keys())\n",
    "normal_train_res_full_std = pd.DataFrame(columns=distributions_full[dist_full].keys())\n",
    "mix_train_res_full_mean = pd.DataFrame(columns=distributions_full[dist_full].keys())\n",
    "mix_train_res_full_std = pd.DataFrame(columns=distributions_full[dist_full].keys())\n",
    "res_full = {\n",
    "    \"train\": (normal_train_res_full_mean, normal_train_res_full_std),\n",
    "    \"mix\": (mix_train_res_full_mean, mix_train_res_full_std)\n",
    "    }\n",
    "\n",
    "targets_full = {\n",
    "    \"train\": np.array([i for i in range(10) for _ in range(nb_poems//10)]),\n",
    "    \"mix\": np.array([i for i in range(10) for _ in range(nb_poems_mix//10)]),\n",
    "    \"test\": np.array([i for i in range(10) for _ in range(nb_poems_test//10)])\n",
    "    \n",
    "}\n",
    "\n",
    "num_iter = 15\n",
    "\n",
    "for dataset, results in res_full.items():\n",
    "    \n",
    "    h0_kernel_train = defaultdict(lambda: {})\n",
    "    h0_kernel_test = defaultdict(lambda: {})\n",
    "\n",
    "    h1_kernel_train = {}\n",
    "    h1_kernel_test = {}\n",
    "\n",
    "    for split in distributions_full[dist_full].keys():\n",
    "        for cm in links:\n",
    "            train_h0_sr = logs_full[dataset][cm][\"h0\"][split]\n",
    "            test_h0_sr = logs_full[\"test\"][cm][\"h0\"][split]\n",
    "            \n",
    "            h0_kernel_train[split][cm] = np.array([[f.dot(g) for g in train_h0_sr] for f in train_h0_sr])\n",
    "            h0_kernel_test[split][cm] = np.array([[f.dot(g) for g in train_h0_sr] for f in test_h0_sr])\n",
    "        \n",
    "        train_h1_sr = logs_full[dataset][\"single\"][\"h1\"][split]\n",
    "        test_h1_sr = logs_full[\"test\"][\"single\"][\"h1\"][split]\n",
    "        \n",
    "        h1_kernel_train[split] = np.array([[f.dot(g) for g in train_h1_sr] for f in train_h1_sr])\n",
    "        h1_kernel_test[split] = np.array([[f.dot(g) for g in train_h1_sr] for f in test_h1_sr])\n",
    "\n",
    "    acc_logs = defaultdict(lambda: [[] for _ in range(num_iter)])\n",
    "    for i in range(num_iter):\n",
    "        for split in distributions_full[dist_full].keys():\n",
    "            acc_h0 = []\n",
    "            clf_h0 = []\n",
    "            \n",
    "            # H1 classifier\n",
    "            clf_h1 =  SVC(kernel='precomputed', probability=True)\n",
    "            clf_h1.fit(h1_kernel_train[split], targets_full[dataset])\n",
    "            prob_h1 = clf_h1.predict_proba(h1_kernel_test[split])\n",
    "            prediction = np.argmax(prob_h1, axis=-1)\n",
    "            acc_logs[\"h1\"][i].append(accuracy_score(targets_full[\"test\"], prediction))\n",
    "            \n",
    "            probs = None\n",
    "            for cm in links:\n",
    "                # H0 classifier\n",
    "                clf = SVC(kernel='precomputed', probability=True)\n",
    "                clf.fit(h0_kernel_train[split][cm], targets_full[dataset])\n",
    "                clf_h0.append(clf)\n",
    "                prob_h0 = clf.predict_proba(h0_kernel_test[split][cm])\n",
    "                \n",
    "                if probs is None:\n",
    "                    probs = np.zeros_like(prob_h0)\n",
    "                    \n",
    "                probs += prob_h0 # (?)\n",
    "                prediction = np.argmax(prob_h0, axis=-1)\n",
    "                acc_logs[f\"h0_{cm}\"][i].append(accuracy_score(targets_full[\"test\"], prediction))\n",
    "                \n",
    "                # Pairwise ensemble\n",
    "                avg_prob = (prob_h1 + prob_h0)/2.0\n",
    "                prediction = np.argmax(avg_prob, axis=-1)\n",
    "                acc_logs[f\"ensb_{cm}\"][i].append(accuracy_score(targets_full[\"test\"], prediction))\n",
    "            \n",
    "            # Full ensemble\n",
    "            avg_prob = (prob_h1 + probs)/(len(links) + 1)\n",
    "            prediction = np.argmax(avg_prob, axis=-1)\n",
    "            acc_logs[f\"full_ensb\"][i].append(accuracy_score(targets_full[\"test\"], prediction))\n",
    "\n",
    "    \n",
    "    for name, acc in acc_logs.items():\n",
    "        results[0].loc[len(results[0].index)] = np.mean(acc, axis=0)\n",
    "        results[1].loc[len(results[1].index)] = np.std(acc, axis=0)\n",
    "        aux = list(results[0].index)\n",
    "        aux[-1] = name\n",
    "        results[0].index = aux\n",
    "        results[1].index = aux\n",
    "    \n",
    "    results[0].to_csv(os.path.join(path_out, f\"res_full_{dist_full}_{dataset}_mean_{num_iter}.csv\"), index=True)\n",
    "    results[1].to_csv(os.path.join(path_out, f\"res_full_{dist_full}_{dataset}_std_{num_iter}.csv\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mean_std(res_full[\"train\"][0], res_full[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_mean_std(res_full[\"mix\"][0], res_full[\"mix\"][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95c1efeb9379afe4535166b7dabdd8cbd79c9c83966898d9af486774d6e7ab86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
